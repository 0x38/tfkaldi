[directories]
#directory where the training data will be retrieved
train_data = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/data/aurora4/train_si84_multi
#directory where the testing data will be retrieved
test_data = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/data/aurora4/test_eval92
#directory where the training features will be stored and retrieved
train_features = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/features/AURORA4/train/fbank
#directory where the testing features will be stored and retrieved
test_features = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/features/AURORA4/test/fbank
#directory where the posteriors of the testing set will be stored and retrieved
test_posteriors = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/features/AURORA4/test/fbank
#directory where the language model will be retrieved
language = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/data/aurora4/lang
#directory where the language model will be retrieved that is used to create the decoding graph
language_test_bg = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/data/aurora4/lang_test_bg
#directory where the all the data from this experiment will be stored (logs, models, ...)
expdir = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/expdir/AURORA4
#path to the kaldi egs folder
kaldi_egs = /esat/spchdisk/scratch/vrenkens/kaldi/egs/wsj/s5


[general]
# number of jobs for GMM training and alignment (not for nnet training)
num_jobs = 16

[features]
#feature type currently only fbank supported
type = fbank
#length of the sliding window (seconds)
winlen = 0.025
#step of the sliding window (seconds)
winstep = 0.01
#number of fbank filters
nfilt = 39
#number of fft bins
nfft = 512
#low cuttof frequency
lowfreq = 0
#hight cutoff frequency, if -1 set to None
highfreq = -1
#premphesis
preemph = 0.97
#include energy in features
include_energy = True
#snip the edges for sliding window
snip_edges = True
#mfcc option: number of cepstrals
numcep = 13
#mfcc option: cepstral lifter (used to scale the mfccs)
ceplifter = 22
#apply mean and variance normalisation
apply_mvn = True
#use the mean and variance normalisation from kaldi (caution: not used in neural net)
kaldi_cmvn = False

[mono_gmm]
#name of the monophone gmm
name = mono_gmm
#command used to train the gmm (see kaldi)
cmd = /esat/spchdisk/scratch/vrenkens/kaldi/egs/wsj/s5/utils/run.pl


[tri_gmm]
#name of the triphone gmm
name = tri_gmm
#command used to train the gmm (see kaldi)
cmd = /esat/spchdisk/scratch/vrenkens/kaldi/egs/wsj/s5/utils/run.pl
#triphone gmm parameters (kaldi)
num_leaves = 2000
tot_gauss = 10000


[nnet]
#name of the neural net
name = relu_nnet_tri
#if you want to use a monophone model, set tot 'True'
monophone = False
#size of the left and right context window
context_width = 5
#number of neurons in the hidden layers
num_hidden_units = 1024
#number of hidden layers
num_hidden_layers = 5
#nonlinearity used currently supported: relu, tanh, sigmoid
nonlin = relu
#leakage of the relu units
relu_leak = 0
#if you want to do l2 normalization after every layer set to 'True' (recomended for unbounded units like relu)
l2_norm = True
#if you want t use dropout set to a value smaller than 1
dropout = 1
#standard deviation of the initial weights
weights_std = 0.01
#standard deviation of the initial biases
biases_std = 0.1
#number of training steps per layer for layer by layer initialization
init_steps = 5
#number of passes over the entire database
num_epochs = 5
#initial learning rate of the neural net
initial_learning_rate = 0.02
#learning rate for the neural net layer by layer initialisation
learning_rate_init = 0.02
#momentum parameter
momentum = 0.8
#exponential weight decay parameter
learning_rate_decay = 0.2
#size of the minibatch (#utterances)
batch_size = 128
#to limit memory ussage (specifically for GPU) the batch can be devided into even smaller batches. The gradient will be calculated by averaging the gradients of all these mini-batches. This value is the size of these mini-batches in number of frames. For optimal speed this value should be set as high as possible without exeeding the memory. The required amount of memory can be roughly computed as : 2 * (input_dim + num_hidden_layers*num_hidden_units + output_dim)*mini_batch_size*bytes_per_value (bytes per value is usualy 4 (32 bit) or sometimes 8 (64 bit). To use the entire batch set to -1
mini_batch_size = 1000
#size of the validation set, set to 0 if you don't want to use one
valid_size = 0
#frequency of evaluating the validation set
valid_frequency = 10
#if you want to adapt the learning rate based on the validation set, set to True
valid_adapt = False
#number of times the learning will retry (with half learning rate) before terminating the training
valid_retries = 3
#number of utterances used to compute the state prior probabilities (needed to compute pseudo likelihoods)
ex_prio = 1024
#starting step, set to -1 to initialize the neural net, if 0 start from initial neural net final will skip nnet training, final-prio will also skip prior calculation
starting_step = -1
#how many steps are taken between two checkpoints
check_freq = 10
#how many checkpoints are kept on disk
check_buffer = 10
#command used in kaldi decoding
cmd = /esat/spchdisk/scratch/vrenkens/kaldi/egs/wsj/s5/utils/run.pl
#you can visualise the progress of the neural net with tensorboard
visualise = True

