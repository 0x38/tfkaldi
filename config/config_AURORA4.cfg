[directories]
#directory where the training data will be retrieved
train_data = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/data/aurora4/train_si84_multi
#directory where the testing data will be retrieved
test_data = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/data/aurora4/test_eval92
#directory where the training features will be stored and retrieved
train_features = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/features/AURORA4/train
#directory where the testing features will be stored and retrieved
test_features = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/features/AURORA4/test
#directory where the language model will be retrieved
language = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/data/aurora4/lang
#directory where the language model will be retrieved that is used to create the decoding graph
language_test = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/data/aurora4/lang_test_tgpr_5k
#directory where the all the data from this experiment will be stored (logs, models, ...)
expdir = /esat/spchdisk/scratch/vrenkens/code/tfkaldi/expdir/AURORA4
#path to the kaldi egs folder
kaldi_egs = /esat/spchdisk/scratch/vrenkens/kaldi/egs/wsj/s5


[general]
# number of jobs for GMM training and alignment (not for nnet training)
num_jobs = 8

[features]
#feature type currently only fbank supported
type = fbank
#length of the sliding window (seconds)
winlen = 0.025
#step of the sliding window (seconds)
winstep = 0.01
#number of fbank filters
nfilt_fbank = 39
#number of fbank filters for MFCC features
nfilt_mfcc = 26
#number of fft bins
nfft = 512
#low cuttof frequency
lowfreq = 0
#hight cutoff frequency, if -1 set to None
highfreq = -1
#premphesis
preemph = 0.97
#include energy in features
include_energy = True
#snip the edges for sliding window
snip_edges = True
#mfcc option: number of cepstrals
numcep = 13
#mfcc option: cepstral lifter (used to scale the mfccs)
ceplifter = 22
#apply mean and variance normalisation
apply_cmvn = True
#use the mean and variance normalisation from kaldi
kaldi_cmvn = False

[mono_gmm]
#name of the monophone gmm
name = mono_gmm
#command used to train the gmm (see kaldi)
cmd = /esat/spchdisk/scratch/vrenkens/kaldi/egs/wsj/s5/utils/run.pl


[tri_gmm]
#name of the triphone gmm
name = tri_gmm
#command used to train the gmm (see kaldi)
cmd = /esat/spchdisk/scratch/vrenkens/kaldi/egs/wsj/s5/utils/run.pl
#triphone gmm parameters (kaldi)
num_leaves = 2000
tot_gauss = 10000

[lda_mllt]
#name of the LDA+MLLT GMM
name = lda_mllt_gmm
#command used to train the lda mllt gmm
cmd = /esat/spchdisk/scratch/vrenkens/kaldi/egs/wsj/s5/utils/run.pl
#LDA+MLLT gmm parameters (kaldi)
num_leaves = 2000
tot_gauss = 10000
#size of the left and right context window
context_width = 3


[nnet]
#name of the neural net
name = relu_nnet_tri
#name of the gmm model used for the alignments
gmm_name = lda_mllt_gmm
#if you're using monophone alignments, set to True
monophone = False
#size of the left and right context window
context_width = 5
#number of neurons in the hidden layers
num_hidden_units = 2048
#number of hidden layers
num_hidden_layers = 5
#nonlinearity used currently supported: relu, tanh, sigmoid
nonlin = relu
#leakage of the relu units
relu_leak = 0
#if you want to do l2 normalization after every layer set to 'True' (recomended for unbounded units like relu)
l2_norm = True
#if you want t use dropout set to a value smaller than 1
dropout = 0.5
#standard deviation of the initial weights
weights_std = 0.001
#standard deviation of the initial biases
biases_std = 0.1
#number of training steps per layer for layer by layer initialization
init_steps = 10
#number of passes over the entire database
num_epochs = 10
#initial learning rate of the neural net
initial_learning_rate = 0.02
#learning rate for the neural net layer by layer initialisation
learning_rate_init = 0.02
#momentum parameter
momentum = 0.8
#exponential weight decay parameter
learning_rate_decay = 01
#size of the minibatch (#utterances)
batch_size = 128
#to limit memory ussage (specifically for GPU) the batch can be devided into even smaller batches. The gradient will be calculated by averaging the gradients of all these mini-batches. This value is the size of these mini-batches in number of frames. For optimal speed this value should be set as high as possible without exeeding the memory. To use the entire batch set to -1
mini_batch_size = 20000
#size of the validation set, set to 0 if you don't want to use one
valid_size = 256
#frequency of evaluating the validation set
valid_frequency = 10
#if you want to adapt the learning rate based on the validation set, set to True
valid_adapt = True
#number of times the learning will retry (with half learning rate) before terminating the training
valid_retries = 3
#number of utterances used to compute the state prior probabilities (needed to compute pseudo likelihoods)
ex_prio = 1024
#starting step, set to -1 to initialize the neural net, if 0 start from initial neural net final will skip nnet training, final-prio will also skip prior calculation
starting_step = -1
#how many steps are taken between two checkpoints
check_freq = 10
#how many checkpoints are kept on disk
check_buffer = 10
#command used in kaldi decoding
cmd = /esat/spchdisk/scratch/vrenkens/kaldi/egs/wsj/s5/utils/run.pl
#you can visualise the progress of the neural net with tensorboard
visualise = True

